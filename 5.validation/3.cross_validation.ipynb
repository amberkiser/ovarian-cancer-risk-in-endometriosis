{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "from nn_load_data import LoadNNData\n",
    "from nn_utils import *\n",
    "from nn_models import *\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_cv(X, y, base_clf, cv_num=10, repeat_num=1):\n",
    "    train_scores = pd.DataFrame()\n",
    "    val_scores = pd.DataFrame()\n",
    "    \n",
    "    for repeat in range(repeat_num):\n",
    "        k_folds = StratifiedKFold(n_splits=cv_num)\n",
    "        fold = 0\n",
    "        for train_index, val_index in k_folds.split(X, y):\n",
    "            fold += 1\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "            clf = clone(base_clf)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            y_train_pred = clf.predict(X_train)\n",
    "            y_train_prob = clf.predict_proba(X_train)\n",
    "            y_train_prob = y_train_prob[:, 1]\n",
    "            train_scores = pd.concat([train_scores, \n",
    "                                      evaluate_cv_results(y_train, y_train_pred, y_train_prob, fold, repeat)])\n",
    "\n",
    "            y_val_pred = clf.predict(X_val)\n",
    "            y_test_prob = clf.predict_proba(X_val)\n",
    "            y_test_prob = y_test_prob[:, 1]\n",
    "            val_scores = pd.concat([val_scores,\n",
    "                                    evaluate_cv_results(y_val, y_val_pred, y_test_prob, fold, repeat)])\n",
    "\n",
    "    return train_scores, val_scores\n",
    "\n",
    "def run_NN_cv(X_NN, y_NN, selected_columns, cv_num=10, repeat_num=1, device='cpu'):\n",
    "    train_scores = pd.DataFrame()\n",
    "    val_scores = pd.DataFrame()\n",
    "    \n",
    "    for repeat in range(repeat_num):\n",
    "        k_folds = StratifiedKFold(n_splits=cv_num)\n",
    "        fold = 0\n",
    "        for train_index, test_index in k_folds.split(X_NN, y_NN):\n",
    "            fold += 1\n",
    "            X_train, X_test = X_NN[train_index], X_NN[test_index]\n",
    "            y_train, y_test = y_NN[train_index], y_NN[test_index]            \n",
    "            X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, stratify=y_train)\n",
    "            \n",
    "            batch_size = 2048\n",
    "            lr = 0.005\n",
    "\n",
    "            number_of_features = len(selected_columns)\n",
    "            input_size = number_of_features\n",
    "            h1_size = 32\n",
    "            h2_size = 16\n",
    "            output_size = 1\n",
    "            epochs = 1000\n",
    "            \n",
    "            # load the data\n",
    "            train_loader = LoadNNData(X_train, y_train, batch_size)\n",
    "            val_loader = LoadNNData(X_val, y_val, batch_size)\n",
    "            test_loader = LoadNNData(X_test, y_test, batch_size)\n",
    "            \n",
    "            # instantiate the model\n",
    "            model = NeuralNetModule(input_size, h1_size, h2_size, output_size)\n",
    "            model = model.double()\n",
    "            model.to(device)\n",
    "            \n",
    "            # set training variables\n",
    "            pos_weight = train_loader.find_pos_weight()\n",
    "            criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "            sigmoid = nn.Sigmoid()\n",
    "            bestValAUC = 0.0\n",
    "            bestValEpoch = 0\n",
    "            patience = 10\n",
    "\n",
    "            # epoch loop\n",
    "            for ep in range(1, epochs + 1):\n",
    "                # batch loop\n",
    "                n_iter = 0\n",
    "                for inputs, labels in train_loader.loader:\n",
    "                    n_iter += 1\n",
    "                    train_batch(model, inputs, labels, device, criterion, optimizer)\n",
    "              \n",
    "                train_auc, val_auc = validate_nn(model, device, train_loader, val_loader, sigmoid)\n",
    "\n",
    "                # early stopping check\n",
    "                if val_auc > bestValAUC:\n",
    "                    bestValAUC = val_auc\n",
    "                    bestValEpoch = ep\n",
    "                    best_model = copy.deepcopy(model)\n",
    "                if ep - bestValEpoch > patience:\n",
    "                    break\n",
    "            \n",
    "            y_true_nn_train, y_prob_nn_train = get_predictions(best_model, device, train_loader, sigmoid)\n",
    "            y_pred_nn_train = np.where(y_prob_nn_train > 0.5, 1, 0)\n",
    "            train_scores = pd.concat([train_scores, \n",
    "                                      evaluate_cv_results(y_true_nn_train, y_pred_nn_train, y_prob_nn_train, \n",
    "                                                          fold, repeat)])\n",
    "            \n",
    "            y_true_nn_test, y_prob_nn_test = get_predictions(best_model, device, test_loader, sigmoid)\n",
    "            y_pred_nn_test = np.where(y_prob_nn_test > 0.5, 1, 0)\n",
    "            val_scores = pd.concat([val_scores,\n",
    "                                    evaluate_cv_results(y_true_nn_test, y_pred_nn_test, y_prob_nn_test, \n",
    "                                                        fold, repeat)])\n",
    "\n",
    "    return train_scores, val_scores\n",
    "\n",
    "def evaluate_cv_results(y_true, y_pred, y_prob, fold, repeat):   \n",
    "    # Performance metrics\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "    sensitivity = recall_score(y_true, y_pred)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Collect metrics in dataframe\n",
    "    scores = pd.DataFrame({'fold': fold,\n",
    "                           'repeat': repeat,\n",
    "                           'AUC': [auc],\n",
    "                           'SENSITIVITY': [sensitivity], \n",
    "                           'SPECIFICITY': [specificity]})\n",
    "\n",
    "    return scores\n",
    "\n",
    "\n",
    "def summarize_cv_results(cv_results):    \n",
    "    alpha = 100-95\n",
    "    metrics = ['AUC','SENSITIVITY','SPECIFICITY']\n",
    "    medians = []\n",
    "    ci_low = []\n",
    "    ci_high = []\n",
    "    \n",
    "    for col in metrics:\n",
    "        medians.append(np.percentile(cv_results[col], 50))\n",
    "        ci_low.append(np.percentile(cv_results[col], alpha/2))\n",
    "        ci_high.append(np.percentile(cv_results[col], 100-alpha/2))\n",
    "\n",
    "    metrics = pd.DataFrame({'METRIC': metrics, 'MEDIAN': medians, 'CI_LOW': ci_low, 'CI_HIGH': ci_high})\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open('X_columns.pkl', 'rb') as f:\n",
    "    X_columns = pickle.load(f)\n",
    "with open('selected_columns_rf.pkl', 'rb') as f:\n",
    "    selected_columns = pickle.load(f)\n",
    "\n",
    "with open('train_X.pkl', 'rb') as f:\n",
    "    X = pickle.load(f)\n",
    "with open('train_y.pkl', 'rb') as f:\n",
    "    y = pickle.load(f)\n",
    "\n",
    "X = pd.DataFrame(X, columns=X_columns)\n",
    "X = X[selected_columns]\n",
    "X = X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB\n",
    "nb_clf = ComplementNB(norm=True)\n",
    "\n",
    "train_cv, val_cv = run_cv(X, y, nb_clf, cv_num=10, repeat_num=5)\n",
    "summarize_cv_results(train_cv).to_csv('nb_train_cv.csv', index=False)\n",
    "summarize_cv_results(val_cv).to_csv('nb_val_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LR\n",
    "lr_clf = LogisticRegression(max_iter=50000,\n",
    "                            solver='liblinear',\n",
    "                            penalty='l1',\n",
    "                            class_weight='balanced',\n",
    "                            C=0.1)\n",
    "\n",
    "train_cv, val_cv = run_cv(X, y, lr_clf, cv_num=10, repeat_num=5)\n",
    "summarize_cv_results(train_cv).to_csv('lr_train_cv.csv', index=False)\n",
    "summarize_cv_results(val_cv).to_csv('lr_val_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "rf_clf = RandomForestClassifier(n_estimators=200,\n",
    "                                max_depth=20,\n",
    "                                criterion='gini',\n",
    "                                class_weight='balanced')\n",
    "\n",
    "train_cv, val_cv = run_cv(X, y, rf_clf, cv_num=10, repeat_num=5)\n",
    "summarize_cv_results(train_cv).to_csv('rf_train_cv.csv', index=False)\n",
    "summarize_cv_results(val_cv).to_csv('rf_val_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN\n",
    "train_cv, val_cv = run_NN_cv(X, y, selected_columns, cv_num=10, repeat_num=5, device='cpu')\n",
    "summarize_cv_results(train_cv).to_csv('nn_train_cv.csv', index=False)\n",
    "summarize_cv_results(val_cv).to_csv('nn_val_cv.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
