{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import roc_auc_score, recall_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrap predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_y.pkl', 'rb') as f:\n",
    "    train_y = pickle.load(f)\n",
    "with open('test_y.pkl', 'rb') as f:\n",
    "    test_y = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_performance_metrics(prediction_data):\n",
    "    # Bootstrap the data\n",
    "    boot_data = resample(prediction_data, stratify=prediction_data['y_true'])\n",
    "\n",
    "    # Performance metrics\n",
    "    auc = roc_auc_score(boot_data['y_true'], boot_data['y_prob'])\n",
    "    sensitivity = recall_score(boot_data['y_true'], boot_data['y_pred'])\n",
    "    tn, fp, fn, tp = confusion_matrix(boot_data['y_true'], boot_data['y_pred']).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Collect metrics in dataframe\n",
    "    bootstrap_df = pd.DataFrame({'AUC': [auc],\n",
    "                                 'SENSITIVITY': [sensitivity],\n",
    "                                 'SPECIFICITY': [specificity]})\n",
    "    return bootstrap_df\n",
    "\n",
    "def summarize_bootstrap_results(bootstrap_results):    \n",
    "    alpha = 100-95\n",
    "    metrics = []\n",
    "    medians = []\n",
    "    ci_low = []\n",
    "    ci_high = []\n",
    "    \n",
    "    for col in bootstrap_results.columns:\n",
    "        metrics.append(col)\n",
    "        medians.append(np.percentile(bootstrap_results[col], 50))\n",
    "        ci_low.append(np.percentile(bootstrap_results[col], alpha/2))\n",
    "        ci_high.append(np.percentile(bootstrap_results[col], 100-alpha/2))\n",
    "\n",
    "    metrics = pd.DataFrame({'METRIC': metrics, 'MEDIAN': medians, 'CI_LOW': ci_low, 'CI_HIGH': ci_high})\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nb_predictions.pkl', 'rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "    \n",
    "\n",
    "nb_predictions_train = pd.DataFrame({'y_true': train_y, \n",
    "                                     'y_pred': predictions['train']['y_pred'], \n",
    "                                     'y_prob': predictions['train']['y_prob']})\n",
    "nb_train_bootstrap_results = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    nb_train_bootstrap_results = pd.concat([nb_train_bootstrap_results, \n",
    "                                            bootstrap_performance_metrics(nb_predictions_train)])\n",
    "nb_train_metrics = summarize_bootstrap_results(nb_train_bootstrap_results)\n",
    "\n",
    "\n",
    "nb_predictions_test = pd.DataFrame({'y_true': test_y, \n",
    "                                    'y_pred': predictions['test']['y_pred'], \n",
    "                                    'y_prob': predictions['test']['y_prob']})\n",
    "nb_test_bootstrap_results = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    nb_test_bootstrap_results = pd.concat([nb_test_bootstrap_results, \n",
    "                                           bootstrap_performance_metrics(nb_predictions_test)])\n",
    "nb_test_metrics = summarize_bootstrap_results(nb_test_bootstrap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lr_predictions.pkl', 'rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "    \n",
    "\n",
    "lr_predictions_train = pd.DataFrame({'y_true': train_y, \n",
    "                                     'y_pred': predictions['train']['y_pred'], \n",
    "                                     'y_prob': predictions['train']['y_prob']})\n",
    "lr_train_bootstrap_results = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    lr_train_bootstrap_results = pd.concat([lr_train_bootstrap_results, \n",
    "                                            bootstrap_performance_metrics(lr_predictions_train)])\n",
    "lr_train_metrics = summarize_bootstrap_results(lr_train_bootstrap_results)\n",
    "\n",
    "\n",
    "lr_predictions_test = pd.DataFrame({'y_true': test_y, \n",
    "                                    'y_pred': predictions['test']['y_pred'], \n",
    "                                    'y_prob': predictions['test']['y_prob']})\n",
    "lr_test_bootstrap_results = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    lr_test_bootstrap_results = pd.concat([lr_test_bootstrap_results, \n",
    "                                           bootstrap_performance_metrics(lr_predictions_test)])\n",
    "lr_test_metrics = summarize_bootstrap_results(lr_test_bootstrap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rf_predictions.pkl', 'rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "    \n",
    "\n",
    "rf_predictions_train = pd.DataFrame({'y_true': train_y, \n",
    "                                     'y_pred': predictions['train']['y_pred'], \n",
    "                                     'y_prob': predictions['train']['y_prob']})\n",
    "rf_train_bootstrap_results = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    rf_train_bootstrap_results = pd.concat([rf_train_bootstrap_results, \n",
    "                                            bootstrap_performance_metrics(rf_predictions_train)])\n",
    "rf_train_metrics = summarize_bootstrap_results(rf_train_bootstrap_results)\n",
    "\n",
    "\n",
    "rf_predictions_test = pd.DataFrame({'y_true': test_y, \n",
    "                                    'y_pred': predictions['test']['y_pred'], \n",
    "                                    'y_prob': predictions['test']['y_prob']})\n",
    "rf_test_bootstrap_results = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    rf_test_bootstrap_results = pd.concat([rf_test_bootstrap_results, \n",
    "                                           bootstrap_performance_metrics(rf_predictions_test)])\n",
    "rf_test_metrics = summarize_bootstrap_results(rf_test_bootstrap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nn_predictions.pkl', 'rb') as f:\n",
    "    predictions = pickle.load(f)\n",
    "    \n",
    "\n",
    "nn_predictions_train = pd.DataFrame({'y_true': predictions['train']['y_true'], \n",
    "                                     'y_pred': predictions['train']['y_pred'], \n",
    "                                     'y_prob': predictions['train']['y_prob']})\n",
    "nn_train_bootstrap_results = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    nn_train_bootstrap_results = pd.concat([nn_train_bootstrap_results, \n",
    "                                            bootstrap_performance_metrics(nn_predictions_train)])\n",
    "nn_train_metrics = summarize_bootstrap_results(nn_train_bootstrap_results)\n",
    "\n",
    "\n",
    "nn_predictions_test = pd.DataFrame({'y_true': predictions['test']['y_true'], \n",
    "                                    'y_pred': predictions['test']['y_pred'], \n",
    "                                    'y_prob': predictions['test']['y_prob']})\n",
    "nn_test_bootstrap_results = pd.DataFrame()\n",
    "for i in range(n):\n",
    "    nn_test_bootstrap_results = pd.concat([nn_test_bootstrap_results, \n",
    "                                           bootstrap_performance_metrics(nn_predictions_test)])\n",
    "nn_test_metrics = summarize_bootstrap_results(nn_test_bootstrap_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_metrics.to_csv('nb_train_metrics.csv', index=False)\n",
    "nb_test_metrics.to_csv('nb_test_metrics.csv', index=False)\n",
    "\n",
    "lr_train_metrics.to_csv('lr_train_metrics.csv', index=False)\n",
    "lr_test_metrics.to_csv('lr_test_metrics.csv', index=False)\n",
    "\n",
    "rf_train_metrics.to_csv('rf_train_metrics.csv', index=False)\n",
    "rf_test_metrics.to_csv('rf_test_metrics.csv', index=False)\n",
    "\n",
    "nn_train_metrics.to_csv('nn_train_metrics.csv', index=False)\n",
    "nn_test_metrics.to_csv('nn_test_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
